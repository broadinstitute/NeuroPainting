---
title: "Select images to print"
---

```{r message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glue)
library(magrittr)
library(stringr)
library(tidyverse)
```

## Prepare load_data 

```{r prepare_load_data, message=FALSE}
load_data <- 
  list.files("load_data_csv/", pattern = "load_data.csv", full.names = T, recursive = T)  %>%
  map_df(read_csv)

convert_to_s3_url <- function(pathname, filename)  {
  file.path(
      str_replace(pathname, 
              "/home/ubuntu/bucket", 
              "https://s3.amazonaws.com/imaging-platform"),
      filename
  )
  
}

channels <- c("DNA", "ER", "RNA", "AGP", "Mito")

for (channel in channels) {
  url_sym <- rlang::sym(str_c("URL_Orig", channel))
  
  path_sym <- rlang::sym(str_c("PathName_Orig", channel))
   
  file_sym <- rlang::sym(str_c("FileName_Orig", channel))
   
  load_data %<>% 
    mutate(!!url_sym := convert_to_s3_url((!!path_sym), (!!file_sym)))
  
}

```

## Get metadata

```{r message=FALSE}

datasets <- 
  tribble(
    ~batch, ~plate,
    "NCP_STEM_1", "BR_NCP_STEM_1",
    "NCP_PROGENITORS_1", "BR_NCP_PROGENITORS_1",
    "NCP_PILOT_3", "BR_NCP_PILOT_3",
    "NCP_PILOT_3B", "MAtt_ICC_test"
  )

metadata <-
  map2_df(datasets$batch,
          datasets$plate,
          function(batch, plate) {
            read_tsv(
              file.path("metadata", glue("{batch}/platemap/{plate}.txt")),
            ) %>%
              distinct()
          })

metadata %<>%
  rename(Plate = plate_map_name, 
         Well = well_position)

names(metadata) <- str_c("Metadata_", names(metadata))
```

## Report plating densities

```{r}
metadata %>% group_by(Metadata_Plate, Metadata_plating_density) %>% tally()
```


## Sample one well per cell line

```{r}
set.seed(5)

metadata_sampled <-
  metadata %>%
  group_by(Metadata_line_ID, Metadata_line_condition, Metadata_compound_ID, Metadata_plating_density) %>%
  sample_n(1) %>%
  ungroup()
```

## Select a fixed field (a.k.a site)

```{r}
fieldIDs <- c(3, 5)
```

## Select images to be printed

```{r}
filenames_header <- paste0("FileName_Orig", channels)

images <- 
  load_data %>%
  select(Metadata_Plate, Metadata_Well, Metadata_Row, Metadata_FieldID, matches("^URL_"), one_of(filenames_header)) %>%
  inner_join(metadata_sampled, by = c("Metadata_Plate", "Metadata_Well")) %>%
  filter(Metadata_FieldID %in% fieldIDs) %>%
  select(matches("^Metadata"), matches("^URL"), matches("^FileName_Orig"))

images_pivoted <-
  images %>%
  select(Metadata_Plate,  Metadata_Well, matches("^URL_")) %>%
  gather(Metadata_Channel, URL, -Metadata_Plate, -Metadata_Well) %>%
  mutate(filename = basename(URL)) %>%
  select(Metadata_Plate, Metadata_Well, Metadata_Channel, filename, URL)

images_pivoted %>% 
  write_csv("data/sample_images.csv")

```

```{r}
images %>%
  select(matches("^Metadata"),  matches("^FileName_Orig")) %>%
  write_csv("data/sample_images_metadata.csv")
```

Run this on command line to download the images:

```{sh eval=FALSE}
IMAGE_DIR=/tmp/ncp

mkdir -p $IMAGE_DIR

cut -d"," -f1 data/sample_images.csv | grep -v Metadata_Plate| sort -u > /tmp/plates.txt

parallel -a /tmp/plates.txt --no-run-if-empty mkdir -p $IMAGE_DIR/{} 

parallel \
 --header ".*\n" \
 -C "," \
 -a data/sample_images.csv \
 --eta \
 --joblog ${IMAGE_DIR}/download.log \
 wget -q -O ${IMAGE_DIR}/{1}/{4} {5}
```

